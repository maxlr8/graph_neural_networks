{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Graph Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Definition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   The main difference between graph data and other types of data is that graph data is structured data. Unlike other data structures which just connect each vertex or node through edges or links, graphs networks store and share characteristc information with them and the neighbouring nodes. They don't have a specific structure, just like a bio-molecule or a protein modelcule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Nodes** are the main data points in a graph network (alike data points in a 2D regression graph) with its own characterics.<br>\n",
    "**2) Edges** are the links that connect the nodes to one another (many to many) based on the specific characteristics (as multiple links) they possess.<br>\n",
    "**3) Features**  are a more recently introduced concept where each node has its own characteristc feature information stored in them.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Based on the EDGES, there are 2 types of graphs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**1) Directed Graphs**\n",
    "\n",
    "    Notation : (A ----> B)\n",
    "    \n",
    "*Directed graphs have uni-directional relationships (i.e) a node A can be related to node B, but not related in case of vice-versa*\n",
    "\n",
    "**2) Un-Directed Graphs**\n",
    "\n",
    "    Notation : (A <----> B) OR (A ---- B)\n",
    "    \n",
    "*Directed graphs have bi-directional relationships (i.e) a node A can be related to node B, and B be related to A in the same manner.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Based on the NODES, there are 2 types of graphs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Homogeneous Graphs**<br>\n",
    "\n",
    "*All the nodes in the graphs have same type of characteristics.*\n",
    "\n",
    "**2) Heterogeneous Graphs**<br>\n",
    "\n",
    "*There is a mixture of types of nodes in the graphs which have different type of characteristics.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Equation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The equation is given by*<br>\n",
    "\n",
    "**G = (V, E, u)**<br>\n",
    "\n",
    "**V** - *Vertices or Nodes*<br>\n",
    "**E** - *Edges or Links =* **(A, W)** - (__Adjancency__ matrix __,__ __Weight__ matrix)<br>\n",
    "**u** - *Feature Vector*<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Intuition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To understand the characteristics of the graph networks intuitively*\n",
    "\n",
    "    Nodes - can be thought as Individual Socail Media Accounts\n",
    "    Edges - can be thought Connections between the friends\n",
    "    Features - can be the Characteristics. Ex - Gender, Age, Followers, etc..\n",
    "    Directed Graphs - can be seen as followers of account A whom A doesn't follow back. Here the matrices are symmetric.\n",
    "    Un-directed Graphs - can be seen as *mutual followers of account A. Here the matrices are asymmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Images/Introduction/Features.jpg\" width=\"1351\" height=\"756\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Storing Graph Information**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Edge Lists**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Edge lists are a single list that contain the edge or link information about the each and every node that they are connected to. They also store the information of the direction of the edge from which node it is going to by specifying the source and target node in the format of \n",
    "**[(Source Node, Target Node)]**\n",
    "\n",
    "    Sample\n",
    "**[(0, 1), (0, 2), (0, 3), (1, 0), (2, 0), (2, 2), (2, 3), (3, 0), (3, 2)]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Images/Introduction/Edge_Lists.jpg\" width=\"1351\" height=\"756\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A and W Matrices**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Adjancency Matrix**\n",
    "\n",
    "    Adjancency matrices also store the information about the edges and the direction in the format of a matrix.\n",
    "\n",
    "|         |  0  |  1  |  2  |  3  |\n",
    "| ------- | --- | --- | --- | --- |\n",
    "|  **0**  |  0  |  1  |  1  |  1  |\n",
    "|  **1**  |  1  |  0  |  0  |  0  |\n",
    "|  **2**  |  1  |  0  |  1  |  1  |\n",
    "|  **3**  |  1  |  0  |  1  |  0  |\n",
    "\n",
    "Adjancency matrices are __symmetric__ if they are from Un-directed graphs, and __asymmetric__ if they are Directed graphs.\n",
    "\n",
    "#### **Weight Matrix**\n",
    "\n",
    "    Similar to adjancy matrices the Weight Matrices store the weights of the edges determining the strength of relations or influence between the each nodes. Hence, these values get replaces with float values.\n",
    "\n",
    "|         |   0   |    1  |   2   |   3   |\n",
    "| ------- | :---: | :---: | :---: | :---: |\n",
    "|  **0**  |  0.0  |  2.0  |  1.5  |  5.0  |\n",
    "|  **1**  |  5.0  |   0   |   0   |   0   |\n",
    "|  **2**  |  1.5  |   0   |   1   |   1   |\n",
    "|  **3**  |  12   |   0   |   1   |   0   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Degree and Laplacian Graphs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **D - Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The D matrix stands for *'Degree'* matrix. It holds the information about the number of connections per node. Assume - \n",
    "    \n",
    "    X - Matrix = Stores all the node values\n",
    "    A - Matrix = Adjacnecy Matrix\n",
    "    D - Matrix = Degree Matrix\n",
    "    \n",
    "    \n",
    "|  X Matrix  |\n",
    "|   :----:   |\n",
    "|    **A**   |\n",
    "|    **B**   |\n",
    "|    **C**   |\n",
    "|    **D**   |\n",
    "|    **E**   |\n",
    "\n",
    "\n",
    "**Adjacency Matrix - A**\n",
    "\n",
    "    The Adjacency Matrix A is written considering the \n",
    "    1) Column headers as Transpose(X)\n",
    "    2) Row headers values as X\n",
    "\n",
    "|       | **A** | **B** | **C** | **D** | **E** |\n",
    "| :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| **A** | **0** | **1** | **1** | **0** | **0** |\n",
    "| **B** | **1** | **0** | **1** | **1** | **0** |\n",
    "| **C** | **0** | **1** | **1** | **0** | **0** |\n",
    "| **D** | **1** | **1** | **0** | **0** | **1** |\n",
    "| **E** | **0** | **1** | **0** | **0** | **1** |\n",
    "\n",
    "**Degree Matrix - D**\n",
    "    \n",
    "    The Degree Matrix is simply the sum of all the values in the Adjacency matrix A aligning diagonally.\n",
    "\n",
    "|       | **A** | **B** | **C** | **D** | **E** |\n",
    "| :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| **A** | **2** | **0** | **0** | **0** | **0** |\n",
    "| **B** | **0** | **3** | **0** | **0** | **0** |\n",
    "| **C** | **0** | **0** | **3** | **0** | **0** |\n",
    "| **D** | **0** | **0** | **0** | **2** | **0** |\n",
    "| **E** | **0** | **0** | **0** | **0** | **2** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Images/Introduction/D_Matrix.jpg\" width=\"1351\" height=\"756\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **L - Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L matrix stands for *'Laplacian'* matrix. It simply gives the *Laplacian of Graph Network*. It similarly holds the influence of nodes with each other alike the **Weight Matrix**, but provides the values in negatives. \n",
    "\n",
    "The L Matrix is the difference of Degree with Adjacencey or Weight Matrix.\n",
    "\n",
    "**L = D - A** *OR* **L = D - W**\n",
    "\n",
    "    Ex - L = D - A\n",
    "\n",
    "|       | **A**  | **B**  | **C**  | **D**  | **E**  |\n",
    "| :---: | :----: | :----: | :----: | :----: | :----: |\n",
    "| **A** | **2**  | **-1** | **-1** | **0**  | **1**  |\n",
    "| **B** | **-1** | **3**  | **-1** | **-1** | **0**  |\n",
    "| **C** | **-1** | **-1** | **3**  | **0**  | **-1** |\n",
    "| **D** | **0**  | **-1** | **0**  | **2**  | **-1** |\n",
    "| **E** | **0**  | **0**  | **-1** | **-1** | **2**  |\n",
    "\n",
    "**NOTE** - The negative values don't indicate negative corelattion, it simply means no influence.\n",
    "\n",
    "    Further, these matrices are normalized for easier computation. Once normalized, the matrix L is writtern as L-bar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Images/Introduction/L_Matrix.jpg\" width=\"1351\" height=\"756\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Learning Process**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Use cases**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*These matrices will serve as an input to the Graph Neural networks. Using them, it is used for other prediction such as*\n",
    "    \n",
    "    1) Node Prediction - Predcition of the position of a new node in the network based on it parameters.\n",
    "    2) Edge Prediction - Preciting new relationships of the nodes with the other nodes in the graph.\n",
    "    3) Graph Prediction - Predicting a new graph network based on our use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Images/Introduction/Applications.jpg\" width=\"1351\" height=\"756\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to analyse the complex structure of graphs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    These generated graph networks are complex and typically have no specific shapes. This very reason helps the network to encompass tremendous amounts of information without being misplaced using the nodes and edges. But the same structure becomes harder to practically analyse and extract the information out of it. Hence, to achieve this, we reduce the dimension of the graphs network by preserving as much information possible.\n",
    "\n",
    "*This is done by converting every node of the graph to a new dimension, which is referred to as **Latin Representation** or **Invading Space**.*\n",
    "\n",
    "Let's considering two nodes from the network, **V** and **U**. Now, on reducing them to *Latin representation*, they will be written as **Zv** and **Zu**. This process is called *__Encoding__*.<br>\n",
    "The goal of this process is to preserve the similarity of **V** with **Zv** and **U** with **Zu** to preserve most of the information.\n",
    "\n",
    "**Similarity_Original_Space(U, V) ~ Similarity_Embedding_Space(Zv, Zu)**\n",
    "    \n",
    "*__So(U, V) ~ Se(Zu, Zv)__*\n",
    "\n",
    "*To achieve this, one of the basic ways to retain the similarity is to find the __Euclidean Distance__ between So and Se values, and minimize this cost function. There are many types of operations performed that is done to retain similarity -* \n",
    "\n",
    "a) **Matrix Factorization** - *Inner product( Transpose(Zu) * Zv* )<br>\n",
    "b) **Look-Up Table** - *Inner product( Transpose(Zu) * Zv* )<br>\n",
    "c) **Random Walks** - *Decode statistics of random walks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Images/Introduction/Encode_and_Decode.jpg\" width=\"1351\" height=\"756\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Drawabacks**\n",
    "\n",
    "There are a few drawbacks of performing these operations\n",
    "\n",
    "**1) No Parameter Sharing** - *Graph networks already share a definite number relation with each and every node. Hence, it contains massive amounts of data which is computationally expensive to perform these operations.*<br>\n",
    "**2) No Semantic Information** - *The new approach of including feature matrices with each node can't be encoded or decoded.*<br>\n",
    "**3) Not Inductive** - *Means that we cannot generalize the output from past outputs. We cannot predict the outcome of an unseen data.*<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Images/Introduction/Drawbacks.jpg\" width=\"1351\" height=\"756\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GCNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    As a result, to erdicate problem faced by GNNs, the method of convolution was induced into the network which was the solution to solave all the 3 problems. Thish led to the representation of neural networks as graphs leading to the creation of GRAPH CONVOLUTIONAL NEURAL NETWORKS (GCNN).\n",
    "\n",
    "*The Convolution operation solves all the 3 problems.*\n",
    "\n",
    "*__1) Parameter Sharing__*\n",
    "\n",
    "*The kernel moving through the image matrix with smaller strides repeatedly uses the same values while generating the feature matrix by going through smaller strides. This enables the causes the information of the image to be repetedly used in different combinations on different positions of the feature matrix, and thus retaining the information of the image.*\n",
    "\n",
    "**NOTE** - \n",
    "\n",
    "    We can note that the size of the kernel and the length of each stride significantly affects the ability to share the parameters at the cost of computation of the images.\n",
    "\n",
    "*__2) Translation Invariant__*\n",
    "\n",
    "*This means to say that the rotation of the image will not affect the capturing of the features of the image.*\n",
    "\n",
    "*__3) Kernel is independent__*\n",
    "\n",
    "*It means that the input parameters (image) is independent of the Kernel's dimensions*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Images/GCNN/GCNN_Benefits.jpg\" width=\"1351\" height=\"756\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GCNN_Challenges**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Since the CNN were used to convolve images with kernel filters with fixed sizes and strides, a graph is highly non-linear data to be fed to the kernel for convolution.\n",
    "\n",
    "**1) Computationally Expensive** - *Since graph networks possess massive amounts of data, it is computationally expensive to perform these operations.*<br><br>\n",
    "**2) Structurally Variable** - *It it is harder to apply convolutions on GNNs as the dimensions are itself variables as they have a fixed shapes of matrices like images to perform convolutions. The shape of the graph can change the order of nodes interracting with the kernel (Homorphism Problem). Also, Heterogenous graphs have different types of nodes within them with a different feature and attributes making it more complex.*<br><br>\n",
    "**3) No Parameter Sharing** - *Unlike convolutions in CNN that provides information from neighbouring cells on every stride of convolution which completely extracts the information of every cell wrt. to every other cell, applying convolutions on GNNs will render it redundant because the the edges (links) already share all the information with each other node.*<br><br>\n",
    "**4) No Semantic Information** - *Since the edges already share the information directly with every other node, there is no chance of semantic information as sliding the filter over the GNNs will simply have to give the same information from point of view or any direction.*<br><br>\n",
    "**5) Highly Descriptive** - *Graphs already describe the infromation of the relations of one to every other node with the distances. Hence, using different kernels with differnet values will be harder to get an inference from the graph.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Images/GCNN/GCNN_Challenges.jpg\" width=\"1351\" height=\"756\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Graphs in the prespective of Signal Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    A general signal represented over a graph is represented with magnitude (y - axis) over time (x - axis). Signals generally occur in equal intervals of time\n",
    "\n",
    "*When a graph is perceived in a signal's prespective, graphs have irregular sampling widths if we consider __EDGES__ as sampling time and if the __NODES__ have multiple features, then there will be multiple signals for a single nodes*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Images/GCNN/GCNN_as_DSP.jpg\" width=\"1351\" height=\"756\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This additional feature of distances seperating every signal with different distances provides a different prespective of information which is valuable. For example*\n",
    "\n",
    "    1) If we consider the (EEG) ElectroEncephaloGraphy signal produced from the brain, we can identify that the similarity in the parrten of variation of signals reduces as the distance of signals from the brain increases.\n",
    "    \n",
    "    2) Similarly, in case of the COVID epedemic, the regions affected shows the spread of disease from the epicentre of the source of the disease, alike divastation of an atomic bomb over an area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Images/GCNN/Similarity_2.jpg\" width=\"1351\" height=\"756\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now that we have a different representation of the graphs to analyse, we need a tool to extract the information from this huge corpuse of graph data. Since, Neural Networks are capable of analysing and comprehending a graph network with massive amounts of data, it can be fed with graph data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Images/GCNN/Similarity_1.jpg\" width=\"1351\" height=\"756\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    But due to the large dimensions of data of graph networks, the traditional ANNs fail to do the job. Hence, the use of CNNs are used to do the job. But as previously seen, there are other challenges that the CNNs face to read the graph data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Images/GCNN/GCNN_Challenge_2.jpg\" width=\"1351\" height=\"756\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Solving the DSP Convolution to Graph Convolution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DIRECTED GRAPH EXAMPLE**\n",
    "    \n",
    "    DSP signals are time series signals. Now, we know that signals are time-shifted over the X-axis.\n",
    "    \n",
    "The DSP equivalent eq. - &nbsp; __y(n) = x(n+t)__.\n",
    "\n",
    "    Now representing a signal as a matrix and performing a matrix shift similarl to match the time shift can create a similar effect on the DSP signal.\n",
    "\n",
    "*The __Shift Matrix__ when designed accordingly can be bring an equivalent change \" __t__ \" in the TIME SERIES from the eq. - &nbsp; __y(n) = x(n+t)__.*\n",
    "*The shape of the __S Matrix__ is shown below.*\n",
    "\n",
    "**Therefore, shifting the signal n-times is - &nbsp; pow(S, n) * x**\n",
    "\n",
    "    Here, if we observe, the S - Matrix is basically the ADJACENCY MATRIX of the graph that equally tallies with the relationship of the nodes\n",
    "    x = [d a  b  c]\n",
    "\n",
    "*The below graph with nodes - &nbsp; __x = [a b c d]__ has been time shifted twice to give an output - &nbsp; __x = [c d a b]__ &nbsp; with an eq. - &nbsp; __pow(S, 2) * x__*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Images/GCNN/DSP_Graph.jpg\" width=\"1351\" height=\"756\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Similarly, considering an arbitrary graph with the given nodes and edges as below, a single shift is performed.\n",
    "    \n",
    "*The nodes are - __y = [a b c d]__*<br>\n",
    "*The shift values - __n = 1__*<br>\n",
    "*The equation is - __pow(S, 1) * y__*<br>\n",
    "*The output is - __y' = [c+d a 0 c]__*\n",
    "\n",
    "So, the interpretation here is that the time-shift method in the DSP is *__mathematically analogous__* to the convolution method in CNN. The only difference is that the kernels have weights in their filter matrix. And hence they perform *__Weighted Shift__*.\n",
    "\n",
    "*Therefore, the graphs can be subjected to __Convolution__, which means that they will be undergoing __Weighted Time-Shifts__!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Images/GCNN/DSP_C_to_GC.jpg\" width=\"1351\" height=\"756\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The value \"K\" in a Weighted Shift operation applied on a Graph network simply defines the number of shift operations to be performed on the graph.\n",
    "\n",
    "__NOTE - Still need to understand how the increase in K-value changes the representation from local to global ?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Images/GCNN/K_in_GCN.jpg\" width=\"1351\" height=\"756\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
